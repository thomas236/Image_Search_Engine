{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eaf3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 21:04:13.334734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import importlib\n",
    "import importlib\n",
    "package = \"icrawler\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "    \n",
    "    \n",
    "package = \"tensorflow\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"matplotlib\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "    \n",
    "\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "except ImportError:\n",
    "    !pip install -U scikit-learn\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import importlib\n",
    "import importlib\n",
    "package = \"icrawler\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "    \n",
    "    \n",
    "package = \"tensorflow\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"matplotlib\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "    \n",
    "\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "except ImportError:\n",
    "    !pip install -U scikit-learn\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import os  # Import the os module to interact with the operating system.\n",
    "import shutil  # Import the shutil module to perform high-level file operations such as copying and deleting.\n",
    "from icrawler.builtin import GoogleImageCrawler  # Import the GoogleImageCrawler class from the icrawler.builtin module to download images from Google.\n",
    "\n",
    "# Define the directory name and the number of images to download\n",
    "query = \"cat\"  # Set a search query for images to download.\n",
    "query_dir_name = \"query\"  # Set the name of the directory to save the downloaded images.\n",
    "num_images_query = 1  # Set the number of images to download.\n",
    "\n",
    "# Delete the directory if it exists\n",
    "if os.path.exists(query_dir_name):  # Check if the directory exists.\n",
    "    shutil.rmtree(query_dir_name)  # If the directory exists, remove it and all its contents.\n",
    "os.mkdir(query_dir_name)  # Create a new directory with the specified name.\n",
    "\n",
    "# Set up the GoogleImageCrawler to download images\n",
    "google_crawler = GoogleImageCrawler(\n",
    "    feeder_threads=1,  # Set the number of threads to use for feeding URLs to the parser.\n",
    "    parser_threads=1,  # Set the number of threads to use for parsing the HTML pages.\n",
    "    downloader_threads=1,  # Set the number of threads to use for downloading the images.\n",
    "    storage={'root_dir': query_dir_name}  # Set the directory to save the downloaded images.\n",
    ")\n",
    "\n",
    "# Start the crawler to download images\n",
    "google_crawler.crawl(\n",
    "    keyword=query,  # Set the search query for images to download.\n",
    "    max_num=num_images_query,  # Set the number of images to download.\n",
    "    min_size=(200, 200),  # Set the minimum size of the images to download.\n",
    "    file_idx_offset='auto'  # Set the index offset for the downloaded images.\n",
    ")\n",
    "\n",
    "# Define the directory name and the number of images to download\n",
    "query = \"dog\"  # Set a search query for images to download.\n",
    "query_dir_name = \"query\"  # Set the name of the directory to save the downloaded images.\n",
    "num_images_query = 1  # Set the number of images to download.\n",
    "\n",
    "# Set up the GoogleImageCrawler to download images\n",
    "google_crawler = GoogleImageCrawler(\n",
    "    feeder_threads=1,  # Set the number of threads to use for feeding URLs to the parser.\n",
    "    parser_threads=1,  # Set the number of threads to use for parsing the HTML pages.\n",
    "    downloader_threads=1,  # Set the number of threads to use for downloading the images.\n",
    "    storage={'root_dir': query_dir_name}  # Set the directory to save the downloaded images.\n",
    ")\n",
    "\n",
    "# Start the crawler to download images\n",
    "google_crawler.crawl(\n",
    "    keyword=query,  # Set the search query for images to download.\n",
    "    max_num=num_images_query,  # Set the number of images to download.\n",
    "    min_size=(200, 200),  # Set the minimum size of the images to download.\n",
    "    file_idx_offset='auto'  # Set the index offset for the downloaded images.\n",
    ")\n",
    "\n",
    "\n",
    "# Define the directory name and the number of images to download\n",
    "query = \"elephant\"  # Set a search query for images to download.\n",
    "query_dir_name = \"query\"  # Set the name of the directory to save the downloaded images.\n",
    "num_images_query = 1  # Set the number of images to download.\n",
    "\n",
    "# Set up the GoogleImageCrawler to download images\n",
    "google_crawler = GoogleImageCrawler(\n",
    "    feeder_threads=1,  # Set the number of threads to use for feeding URLs to the parser.\n",
    "    parser_threads=1,  # Set the number of threads to use for parsing the HTML pages.\n",
    "    downloader_threads=1,  # Set the number of threads to use for downloading the images.\n",
    "    storage={'root_dir': query_dir_name}  # Set the directory to save the downloaded images.\n",
    ")\n",
    "\n",
    "# Start the crawler to download images\n",
    "google_crawler.crawl(\n",
    "    keyword=query,  # Set the search query for images to download.\n",
    "    max_num=num_images_query,  # Set the number of images to download.\n",
    "    min_size=(200, 200),  # Set the minimum size of the images to download.\n",
    "    file_idx_offset='auto'  # Set the index offset for the downloaded images.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os                    # Import the os module to interact with the operating system.\n",
    "import shutil                # Import the shutil module to perform high-level file operations such as copying and deleting.\n",
    "from icrawler.builtin import GoogleImageCrawler   # Import the GoogleImageCrawler class from the icrawler.builtin module to download images from Google.\n",
    "\n",
    "num_images = 50 # Set the default number of images to download.\n",
    "\n",
    "# Define a function to download images.\n",
    "def download_images(dir_name, num_images = 5):\n",
    "    # Delete the directory if it exists.\n",
    "    if os.path.exists(dir_name):\n",
    "        shutil.rmtree(dir_name)\n",
    "    # Create a new directory with the specified name.\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "    # Set up the GoogleImageCrawler to download images.\n",
    "    google_crawler = GoogleImageCrawler(\n",
    "        feeder_threads=1,\n",
    "        parser_threads=1,\n",
    "        downloader_threads=4,\n",
    "        storage={'root_dir': dir_name}\n",
    "    )\n",
    "\n",
    "    # Start the crawler to download images with the specified parameters.\n",
    "    google_crawler.crawl(\n",
    "        keyword=dir_name,\n",
    "        max_num=num_images,\n",
    "        min_size=(200, 200),\n",
    "        file_idx_offset='auto'\n",
    "    )\n",
    "\n",
    "# Create a list of directory names to download images for.\n",
    "#dir_names = [ \"kitten\",\"dog\",\"monkey\",\"rabbit\",\"deer\",\"giraffe\", \"elephant\", \"rhino\", \"lion\", \"tiger\" ,\"dinosaur\",\"panda\",\"unicorn\",\"dragonfly\",\"mammoth\",\"squirrell\",\"porcupine\",\"merlion\",\"mermaid\"]\n",
    "dir_names = [ \"kitten\",\"dog\",\"rabbit\",\"deer\",\"giraffe\", \"elephant\", \"rhino\", \"lion\", \"tiger\" ,\"dinosaur\",\"panda\",\"unicorn\",\"dragonfly\",\"mammoth\",\"squirrell\",\"porcupine\"]\n",
    "#dir_names = [ \"kitten\",\"dog\",\"monkey\"]\n",
    "\n",
    "# Loop over the directory names and download images for each one.\n",
    "for i in dir_names:\n",
    "    download_images(i,num_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632369be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os          # Import the os module to interact with the operating system.\n",
    "import shutil      # Import the shutil module to perform high-level file operations such as copying and deleting.\n",
    "\n",
    "new_dir_name = \"animals\"         # Define the name of the new directory to move files to.\n",
    "\n",
    "# Delete the directory if it exists, and create a new empty directory.\n",
    "shutil.rmtree(new_dir_name, ignore_errors=True)  \n",
    "os.makedirs(new_dir_name)\n",
    "\n",
    "# Loop through each directory and move the files to the new directory.\n",
    "file_num = 1    # Start numbering the files at 1.\n",
    "for dir_name in dir_names:\n",
    "    for file_name in sorted(os.listdir(dir_name)):   # Loop through each file in the directory, sorted in ascending order.\n",
    "        old_path = os.path.join(dir_name, file_name)   # Get the full path to the old file.\n",
    "        new_file_name = f\"{file_num:04d}_{file_name}\"   # Use leading zeros for file numbering.\n",
    "        new_path = os.path.join(new_dir_name, new_file_name)   # Get the full path to the new file.\n",
    "        shutil.copyfile(old_path, new_path)   # Copy the file to the new directory.\n",
    "        file_num += 1    # Increment the file number for the next file.\n",
    "\n",
    "dir_path = \"animals\"\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "# Loop through each file and delete files that are not PNG or JPG files\n",
    "for file_name in file_list:\n",
    "    if not file_name.endswith(\".png\") and not file_name.endswith(\".jpg\"):\n",
    "        os.remove(os.path.join(dir_path, file_name))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f573bfd8",
   "metadata": {},
   "source": [
    "\n",
    "import os                    # Import the os module to interact with the operating system.\n",
    "import tensorflow as tf      # Import the TensorFlow module.\n",
    "import numpy as np           # Import the NumPy module.\n",
    "import csv                   # Import the CSV module.\n",
    "\n",
    "# Load the InceptionV3 model.\n",
    "model = tf.keras.applications.InceptionV3()\n",
    "\n",
    "# Define the size of the input images.\n",
    "img_size = (299, 299)\n",
    "\n",
    "# Define a function to preprocess the input images.\n",
    "def preprocess_img(img_path):\n",
    "    # Load the image and resize it to the specified size.\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
    "    # Convert the image to a NumPy array.\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    # Preprocess the image to make it compatible with the InceptionV3 model.\n",
    "    x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Define a function to get the predicted labels for the input images.\n",
    "def get_labels(img_path):\n",
    "    # Preprocess the image.\n",
    "    x = preprocess_img(img_path)\n",
    "    # Add an extra dimension to the image.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # Get the predicted probabilities for the image.\n",
    "    preds = model.predict(x)\n",
    "    # Get the top three predicted labels.\n",
    "    labels = tf.keras.applications.inception_v3.decode_predictions(preds, top=3)[0]\n",
    "    return [label[1] for label in labels]\n",
    "\n",
    "# Get the file names of the images in the \"animals\" directory.\n",
    "img_dir = \"animals\"\n",
    "img_names = sorted(os.listdir(img_dir))\n",
    "\n",
    "# Get the predicted labels for each image.\n",
    "image_surrogates = {}\n",
    "for img_name in img_names:\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    labels = get_labels(img_path)\n",
    "    image_surrogates[img_name] = labels\n",
    "\n",
    "# Print the image surrogates.\n",
    "for img_name, labels in image_surrogates.items():\n",
    "    print(f\"{img_name}: {labels}\")\n",
    "\n",
    "# Write the image surrogates to a CSV file.\n",
    "with open(\"textual_surrogate.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"img_name\", \"labels\"])\n",
    "\n",
    "    for img_name, labels in image_surrogates.items():\n",
    "        labels_str = \", \".join(str(label) for label in labels)\n",
    "        writer.writerow([img_name, labels_str])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18447bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                    # Import the os module to interact with the operating system.\n",
    "import tensorflow as tf      # Import the TensorFlow module.\n",
    "import numpy as np           # Import the NumPy module.\n",
    "import csv                   # Import the CSV module.\n",
    "\n",
    "# Load the InceptionV3 model.\n",
    "model = tf.keras.applications.InceptionV3()\n",
    "\n",
    "# Define the size of the input images.\n",
    "img_size = (299, 299)\n",
    "\n",
    "# Define a function to preprocess the input images.\n",
    "def preprocess_img(img_path):\n",
    "    # Load the image and resize it to the specified size.\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
    "    # Convert the image to a NumPy array.\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    # Preprocess the image to make it compatible with the InceptionV3 model.\n",
    "    x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Define a function to get the predicted labels for the input images.\n",
    "def get_labels(img_path):\n",
    "    # Preprocess the image.\n",
    "    x = preprocess_img(img_path)\n",
    "    # Add an extra dimension to the image.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # Get the predicted probabilities for the image.\n",
    "    preds = model.predict(x)\n",
    "    # Get the top three predicted labels.\n",
    "    labels = tf.keras.applications.inception_v3.decode_predictions(preds, top=3)[0]\n",
    "    return [label[1] for label in labels]\n",
    "\n",
    "# Get the file names of the images in the \"animals\" directory.\n",
    "img_dir = \"animals\"\n",
    "img_names = sorted(os.listdir(img_dir))\n",
    "\n",
    "# Write the image surrogates to a CSV file.\n",
    "with open(\"textual_surrogate.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"img_name\", \"labels\"])\n",
    "\n",
    "    # Get the predicted labels for each image and write to CSV file.\n",
    "    for img_name in img_names:\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        labels = get_labels(img_path)\n",
    "        writer.writerow([img_name, labels])\n",
    "\n",
    "        # Print the image surrogates.\n",
    "        print(f\"{img_name}: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a6177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os                                # Import the os module to interact with the operating system.\n",
    "import numpy as np                       # Import the numpy library for numerical operations.\n",
    "import tensorflow as tf                  # Import the TensorFlow library for deep learning computations.\n",
    "import matplotlib.pyplot as plt          # Import the matplotlib library for data visualization.\n",
    "\n",
    "# Load the InceptionV3 model\n",
    "model = tf.keras.applications.InceptionV3(include_top=False, pooling='avg')\n",
    "\n",
    "# Define the size of the input images\n",
    "img_size = (299, 299)\n",
    "\n",
    "# Define a function to preprocess the input images\n",
    "def preprocess_img(img_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    x = tf.keras.applications.inception_v3.preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Define a function to compute feature vectors for a list of images\n",
    "def compute_features(dir_name, names):\n",
    "    features = {}\n",
    "    for name in names:\n",
    "        path = os.path.join(dir_name, name)\n",
    "        img = preprocess_img(path)\n",
    "        feature_vector = model.predict(np.expand_dims(img, axis=0)).flatten()\n",
    "        features[name] = feature_vector\n",
    "    return features\n",
    "\n",
    "# Get the file names of the images in the \"query\" directory\n",
    "query_dir = query_dir_name\n",
    "query_names = sorted(os.listdir(query_dir))\n",
    "\n",
    "# Get the file names of the images in the \"animals\" directory\n",
    "animals_dir = \"animals\"\n",
    "animals_names = sorted(os.listdir(animals_dir))\n",
    "\n",
    "# Compute the feature vectors for all the images in the \"animals\" directory\n",
    "animals_features = compute_features(animals_dir, animals_names)\n",
    "\n",
    "# Compute the feature vectors for all the images in the \"query\" directory\n",
    "query_features = compute_features(query_dir, query_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Number of query files \n",
    "files_query = os.listdir(query)\n",
    "\n",
    "# Number of reference files \n",
    "num_files_query = len(files_query)\n",
    "\n",
    "print(f\"The number of query files: {num_files_query}\")\n",
    "\n",
    "files_reference = os.listdir(new_dir_name)\n",
    "num_files_reference = len(files_reference)\n",
    "\n",
    "print(f\"The number of reference files: {num_files_reference}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e51742f",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity    # Import the cosine_similarity function from the sklearn.metrics.pairwise module.\n",
    "\n",
    "# Compute the cosine similarity matrix between the images in the \"query\" and \"animals\" directories.\n",
    "similarity_matrix = np.zeros((len(query_names), len(animals_names)))   # Create a numpy array with dimensions (len(query_names), len(animals_names)) filled with zeros.\n",
    "for i in range(len(query_names)):\n",
    "    query_name = query_names[i]  # Get the name of the query image at index i.\n",
    "    query_feature = query_features[query_name]   # Get the feature vector for the query image.\n",
    "    for j in range(len(animals_names)):\n",
    "        animals_name = animals_names[j]  # Get the name of the animals image at index j.\n",
    "        animals_feature = animals_features[animals_name]   # Get the feature vector for the animals image.\n",
    "        similarity = cosine_similarity([query_feature], [animals_feature])[0][0]    # Compute the cosine similarity between the query and animals images.\n",
    "        similarity_matrix[i, j] = similarity   # Store the similarity value in the similarity matrix.\n",
    "\n",
    "# Get the top 5 similar images to each image in the \"query\" directory.\n",
    "top_similarities = np.argsort(similarity_matrix, axis=1)[:, -6:-1]   # Get the indices of the top 5 most similar images for each query image.\n",
    "\n",
    "# Display the top 5 similar images and similarity index for each image in the \"query\" directory.\n",
    "for i, query_name in enumerate(query_names):\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(15, 15))   # Create a figure with 6 subplots, one for the query image and 5 for the top 5 most similar images.\n",
    "    # Show the query image.\n",
    "    query_path = os.path.join(query_dir, query_name)   # Get the path to the query image.\n",
    "    query_img = plt.imread(query_path)   # Read the query image.\n",
    "    axs[0].imshow(query_img)   # Show the query image in the first subplot.\n",
    "    axs[0].set_title(\"Query Image\")   # Set the title of the first subplot.\n",
    "    axs[0].axis('off')   # Turn off the axis for the first subplot.\n",
    "    # Show the top 5 similar images.\n",
    "    for j, index in enumerate(top_similarities[i]):\n",
    "        animals_name = animals_names[index]   # Get the name of the animals image at the current index.\n",
    "        similarity = similarity_matrix[i, index]   # Get the similarity value for the current animals image.\n",
    "        path = os.path.join(animals_dir, animals_name)   # Get the path to the animals image.\n",
    "        img = plt.imread(path)   # Read the animals image.\n",
    "        axs[j+1].imshow(img)   # Show the animals image in the current subplot. \n",
    "        axs[j+1].set_title(f\"{animals_name}\\nSimilarity index: {similarity:.3f}\")   # Set the title of the current subplot.\n",
    "        axs[j+1].axis('off')   # Turn off the axis for the current subplot.\n",
    "    plt.suptitle(f\"Top 5 similar images to {query_name}\")   # Set the overall title for the figure.\n",
    "    plt.show()   # Display the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26579cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity    # Import the cosine_similarity function from the sklearn.metrics.pairwise module.\n",
    "\n",
    "# Compute the cosine similarity matrix between the images in the \"query\" and \"animals\" directories.\n",
    "similarity_matrix = np.zeros((len(query_names), len(animals_names)))   # Create a numpy array with dimensions (len(query_names), len(animals_names)) filled with zeros.\n",
    "for i in range(len(query_names)):\n",
    "    query_name = query_names[i]  # Get the name of the query image at index i.\n",
    "    query_feature = query_features[query_name]   # Get the feature vector for the query image.\n",
    "    for j in range(len(animals_names)):\n",
    "        animals_name = animals_names[j]  # Get the name of the animals image at index j.\n",
    "        animals_feature = animals_features[animals_name]   # Get the feature vector for the animals image.\n",
    "        similarity = cosine_similarity([query_feature], [animals_feature])[0][0]    # Compute the cosine similarity between the query and animals images.\n",
    "        similarity_matrix[i, j] = similarity   # Store the similarity value in the similarity matrix.\n",
    "\n",
    "# Get the top 5 similar images to each image in the \"query\" directory.\n",
    "top_similarities = np.argsort(similarity_matrix, axis=1)[:, -6:-1]   # Get the indices of the top 5 most similar images for each query image.\n",
    "\n",
    "\n",
    "# Display the top 5 similar images and similarity index for each image in the \"query\" directory.\n",
    "for i, query_name in enumerate(query_names):\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(15, 15))   # Create a figure with 6 subplots, one for the query image and 5 for the top 5 most similar images.\n",
    "    # Show the query image.\n",
    "    query_path = os.path.join(query_dir, query_name)   # Get the path to the query image.\n",
    "    axs[0].imshow(plt.imread(query_path))   # Show the query image in the first subplot.\n",
    "    axs[0].set_title(\"Query Image\")   # Set the title of the first subplot.\n",
    "    axs[0].axis('off')   # Turn off the axis for the first subplot.\n",
    "    # Show the top 5 similar images.\n",
    "    for j, index in enumerate(top_similarities[i]):\n",
    "        animals_name = animals_names[index]   # Get the name of the animals image at the current index.\n",
    "        similarity = similarity_matrix[i, index]   # Get the similarity value for the current animals image.\n",
    "        path = os.path.join(animals_dir, animals_name)   # Get the path to the animals image.\n",
    "        axs[j+1].imshow(plt.imread(path))   # Show the animals image in the current subplot. \n",
    "        axs[j+1].set_title(f\"{animals_name}\\nSimilarity index: {similarity:.3f}\")   # Set the title of the current subplot.\n",
    "        axs[j+1].axis('off')   # Turn off the axis for the current subplot.\n",
    "    plt.suptitle(f\"Top 5 similar images to {query_name}\")   # Set the overall title for the figure.\n",
    "    plt.show()   # Display the figure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0104d",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
